{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9486a81",
      "metadata": {
        "id": "f9486a81"
      },
      "source": [
        "# Q1. What is an activation function in the context of artificial neural networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9020b859",
      "metadata": {
        "id": "9020b859"
      },
      "source": [
        "In the context of ANN, an activation function is a mathematical function applied\n",
        "to a nodes input to produce an output signal.It determines whether\n",
        "a neuron should be activated or not.This is crucial for the network to learn complex\n",
        "patterns,perfrom non-linear transformations,and model data relationships.\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "59229308",
      "metadata": {
        "id": "59229308"
      },
      "source": [
        "Key Roles of Activation Function:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a99dd9",
      "metadata": {
        "id": "96a99dd9"
      },
      "source": [
        "1) Non-Linearity\n",
        "2) Signal Modulation\n",
        "3) Gradient Propogation"
      ]
    },
    {
      "cell_type": "raw",
      "id": "c6838e1a",
      "metadata": {
        "id": "c6838e1a"
      },
      "source": [
        "Common Activation Functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b52231",
      "metadata": {
        "id": "50b52231"
      },
      "source": [
        "1) Sigmoid\n",
        "2) Tanh (Hyperbolic Tangent)\n",
        "3) ReLu(Rectified Linear Unit)\n",
        "4) Leaky ReLu\n",
        "5) Softmax Activation FUnction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2177809f",
      "metadata": {
        "id": "2177809f"
      },
      "source": [
        "# Q2. What are some common types of activation functions used in neural networks?"
      ]
    },
    {
      "cell_type": "raw",
      "id": "9c75786c",
      "metadata": {
        "id": "9c75786c"
      },
      "source": [
        "Sigmoid\n",
        "Tanh (Hyperbolic Tangent)\n",
        "# ReLu(Rectified Linear Unit)\n",
        "Leaky ReLu\n",
        "\n",
        "Softmax Activation FUnction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe05fba",
      "metadata": {
        "id": "6fe05fba"
      },
      "source": [
        "# Q3. How do activation functions affect the training process and performance of a neural network?"
      ]
    },
    {
      "cell_type": "raw",
      "id": "6a7bea8d",
      "metadata": {
        "id": "6a7bea8d"
      },
      "source": [
        "Here are several ways they influence these aspects:\n",
        "    1) Non Linearity:\n",
        "     Activation function introduce non-linearity to the\n",
        "    network,allowing it to model complex relationships\n",
        "    in the data.Without,non-linear function , a nueral\n",
        "    network would simply behave behave like a linear model.\n",
        "\n",
        "    2) Gradient Progogation :\n",
        "During training ,the gradients and calculated and progogated back through the network to update the weights.A.F affects how gradients flow through the network. Functions like ReLu and its variants help mitigate the vanishing gradient problem,where gradient become too small for effective learning in deep networks.\n",
        "\n",
        "\n",
        "    3) Sparsity :\n",
        "ReLu and its variants introduce sparsity to the network by outputting zero to the negative input values.This sparsity can lead to more efficient computations and help mitigate overfitting.\n",
        "\n",
        "Conclusion:\n",
        "   Choosing the  right activation function is critical      for optimizing the training process and achieveing      the best performance.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f47f3e3",
      "metadata": {
        "id": "6f47f3e3"
      },
      "source": [
        "# Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
      ]
    },
    {
      "cell_type": "raw",
      "id": "4236a60d",
      "metadata": {
        "id": "4236a60d"
      },
      "source": [
        "# The sigmoid activation function is a mathematical function used in neural networks to introduce non-linearity into the model. It is defined by the formula:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81268ff",
      "metadata": {
        "id": "a81268ff"
      },
      "source": [
        "# ‚ÄúœÉ(x)=1/1+e^‚àíx‚Äã‚Äù"
      ]
    },
    {
      "cell_type": "raw",
      "id": "11a89afc",
      "metadata": {
        "id": "11a89afc"
      },
      "source": [
        "Input Range : The input x can raange from -inf to +inf.\n",
        "Output Range : It lies between 0 and 1.\n",
        "S-Shape Curve : The function has an S shaped curve,\n",
        "                 also known as logistic curve.\n",
        "Saturation : For very high positive inputs,the output\n",
        "#             approaches 1 and for very low it app. *0*\n",
        ""
      ]
    },
    {
      "cell_type": "raw",
      "id": "dbf8817b",
      "metadata": {
        "id": "dbf8817b"
      },
      "source": [
        "ADVANTAGES :\n",
        "1) Probabilistic Interpretation -> Since the output lies\n",
        "in the range (0,1) it can be interpreted as probability.\n",
        "This makes this fucntion useful in binary classification\n",
        "problems.\n",
        "2) Smooth Gradient -> The function is smmoth and diffe-\n",
        "entiable,which makes it better for gradient-based\n",
        "optimization methods.\n",
        "3) Monotonic -> This function is monotonic ,which means\n",
        "#   it consistently increases,making it easier to understand\n",
        "how changes in input affect the output."
      ]
    },
    {
      "cell_type": "raw",
      "id": "e632491e",
      "metadata": {
        "id": "e632491e"
      },
      "source": [
        "Disadvantages:\n",
        "1)Vanishing Gradient Problem: When the input ùë• is very large\n",
        "or very small, the gradient (derivative) of the sigmoid\n",
        "function becomes very small, leading to very small updates\n",
        "to the weights during backpropagation. This can slow down\n",
        "the training process significantly.\n",
        "2)Not Zero-Centered: The output range is (0, 1), which means\n",
        "the activations are always positive. This can cause\n",
        "the gradients to have a consistent direction during\n",
        "weight updates, leading to inefficient training.\n",
        "3) COmputationally Expensive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5716d601",
      "metadata": {
        "id": "5716d601"
      },
      "source": [
        "# Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b71124ac",
      "metadata": {
        "id": "b71124ac"
      },
      "source": [
        "ReLu activation function is defined as\n",
        "ReLu(x) = max(0,x)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "5bf3bbd4",
      "metadata": {
        "id": "5bf3bbd4"
      },
      "source": [
        "Differences from the sigmoid function ->\n",
        " 1) Relu outputs the input directly if it is positive,\n",
        "    otherwise it outputs zero  whereas sigmoid outputs\n",
        "    between 0 and 1 using formula 1/1+e^-x\n",
        "2) Output range of  ReLu is [0, infinity]\n",
        "   whereas that of Sigmoid is (0,1)\n",
        "3) ReLu is computationally simpler and faster whereas\n",
        "   Sigmoid is more computationally intensive due to\n",
        "    exponential fucntion\n",
        "4) ReLu is not zero-centered but Sigmoid is zero-centered meaning always positive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e7ecaea",
      "metadata": {
        "id": "4e7ecaea"
      },
      "source": [
        "In summary, ReLU is preferred for deep networks due to its simplicity, efficiency, and ability to mitigate the vanishing gradient problem, whereas the sigmoid function is useful for binary classification but suffers from vanishing gradients and slower training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0650f43d",
      "metadata": {
        "id": "0650f43d"
      },
      "source": [
        "# Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
      ]
    },
    {
      "cell_type": "raw",
      "id": "58b0df21",
      "metadata": {
        "id": "58b0df21"
      },
      "source": [
        "1) Mitigates Vanishing gradient Problem ->\n",
        "ReLu's gradient is either 0 or 1 ,avoiding very small\n",
        "gradients that slow training.\n",
        "2) Computationally Efficient ->\n",
        "It is simpler and faster to compute.\n",
        "3) Sparsity : Outputs zero for negative inputs ,\n",
        "0   improving efficiency and reducing model complexity.\n",
        "4) Faster Convergence -> It has quick training and\n",
        "convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1be7da1",
      "metadata": {
        "id": "a1be7da1"
      },
      "source": [
        "# Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ee03be",
      "metadata": {
        "id": "f0ee03be"
      },
      "source": [
        "Leaky ReLu is a variant of the rectified linear unit\n",
        "activation function that allows a small non-zero\n",
        "gradient when the input is negative.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f32a68fe",
      "metadata": {
        "id": "f32a68fe"
      },
      "source": [
        "# Leaky ReLu(x) = max( 0.01x,x)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "2faec9d5",
      "metadata": {
        "id": "2faec9d5"
      },
      "source": [
        "Leaky ReLU avoids zero gradients for negative inputs, unlike traditional ReLU, which could result in dead neurons. This small gradient helps maintain a flow of information and avoids complete inactivation of neurons, thereby mitigating the vanishing gradient problem commonly seen in deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b0e68e",
      "metadata": {
        "id": "12b0e68e"
      },
      "source": [
        "# Q8. What is the purpose of the softmax activation function? When is it commonly used?"
      ]
    },
    {
      "cell_type": "raw",
      "id": "66a5c50f",
      "metadata": {
        "id": "66a5c50f"
      },
      "source": [
        "The purpose of the softmax activation function is to convert logits into probabilities, making the output vector's components sum to 1.\n",
        "\n",
        "It is commonly used in the output layer of neural networks for multi-class classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e66d4c",
      "metadata": {
        "id": "24e66d4c"
      },
      "source": [
        "# Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ddc1b3e",
      "metadata": {
        "id": "0ddc1b3e"
      },
      "source": [
        "The hyperbolic tangent (tanh) activation function is defined as:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af76bb9",
      "metadata": {
        "id": "4af76bb9"
      },
      "source": [
        "# ‚Äútanh(x)=e^x - e^‚àíx  / e^x+e‚àíx‚Äã‚Äù"
      ]
    },
    {
      "cell_type": "raw",
      "id": "c39705f2",
      "metadata": {
        "id": "c39705f2"
      },
      "source": [
        "Comparison to the Sigmoid Function:\n",
        "***Range**: Tanh outputs values between -1 and 1, whereas sigmoid outputs values between 0 and 1.\n",
        "\n",
        "**Zero-Centered**: Tanh is zero-centered (mean is zero), while sigmoid is not.\n",
        "**Gradient**: Tanh has stronger gradients than sigmoid,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80cbb647",
      "metadata": {
        "id": "80cbb647"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}